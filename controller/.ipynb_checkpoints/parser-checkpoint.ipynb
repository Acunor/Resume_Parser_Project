{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118f30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import FileFetcher as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5bdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "         \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def parse_text(self,text):\n",
    "        pass\n",
    "    \n",
    "    def extract_name(self,name):   \n",
    "        nlp = spacy.load('en_core_web_sm') # load pre-trained model\n",
    "        matcher = Matcher(nlp.vocab)       # initialize matcher with a vocab\n",
    "        nlp_text = nlp(name)\n",
    "        pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]  # First name and Last name are always Proper Nouns\n",
    "        matcher.add('NAME',[pattern])       \n",
    "        matches = matcher(nlp_text)\n",
    "        for match_id, start, end in matches:\n",
    "            span = nlp_text[start:end]\n",
    "            return span.text\n",
    "        \n",
    "    def extract_mobile_number(self,mobile):\n",
    "        phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), mobile)\n",
    "        if phone:\n",
    "            number = ''.join(phone[0])\n",
    "            if len(number) > 10:\n",
    "                return '+' + number\n",
    "            else:\n",
    "                return number\n",
    "            \n",
    "    def extract_email(self,email):\n",
    "        email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "        if email:\n",
    "            try:\n",
    "                return email[0].split()[0].strip(';')\n",
    "            except IndexError:\n",
    "                return None\n",
    "       \n",
    "    def extract_skills(self,skills):\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        word_tokens = nltk.tokenize.word_tokenize(skills)\n",
    "        # remove the stop words\n",
    "        filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "        # remove the punctuation\n",
    "        filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "        # generate bigrams and trigrams (such as artificial intelligence)\n",
    "        bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "        # we create a set to keep the results in.\n",
    "        found_skills = set()\n",
    "        # we search for each token in our skills database\n",
    "        for token in filtered_tokens:\n",
    "            if token.lower() in SKILLS_DB:\n",
    "                found_skills.add(token)\n",
    "        # we search for each bigram and trigram in our skills database\n",
    "        for ngram in bigrams_trigrams:\n",
    "            if ngram.lower() in SKILLS_DB:\n",
    "                found_skills.add(ngram)\n",
    "        return found_skills\n",
    "    \n",
    "    def extract_education(self,edu):\n",
    "        nlp = spacy.load('en_core_web_sm')  # load pre-trained model\n",
    "        STOPWORDS = set(stopwords.words('english'))  # Grad all general stop words\n",
    "        EDUCATION = ['BE','B.E.','B.E','BS','B.S','ME','M.E','M.E.','MS','M.S','BTECH','B.TECH','M.TECH','MTECH','SSC','HSC','CBSE','ICSE','X','XII']\n",
    "        nlp_text = nlp(edu)\n",
    "        # Sentence Tokenizer\n",
    "        #nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "        nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "        edu = {}\n",
    "        # Extract education degree\n",
    "        for index, text in enumerate(nlp_text):\n",
    "            for tex in text.split():\n",
    "                # Replace all special symbols\n",
    "                tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "                if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                    edu[tex] = text + nlp_text[index + 1]\n",
    "        # Extract year\n",
    "        education = []\n",
    "        for key in edu.keys():\n",
    "            year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "            if year:\n",
    "                education.append((key, ''.join(year[0])))\n",
    "            else:\n",
    "                education.append(key)\n",
    "        return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e96dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandra Shankar\n",
      "2674371641\n",
      "vmcshankar@gmail.com\n",
      "{'SQL', 'Agile', 'Azure', 'agile', 'Jira', 'JIRA'}\n",
      "['MS', 'Be']\n"
     ]
    }
   ],
   "source": [
    "SKILLS_DB = ['linq','sql server','agile','scrun','waterfall','jams','azure','sql','jira','spotlight']\n",
    "resume_text=ff.FileFetchers.fetch_pdf_file('self')\n",
    "par=Parser()\n",
    "print(par.extract_name(resume_text))\n",
    "print(par.extract_mobile_number(resume_text))\n",
    "print(par.extract_email(resume_text))\n",
    "print(par.extract_skills(resume_text))\n",
    "print(par.extract_education(resume_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a8d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966acce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26837548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0eb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3e95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebcf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee8fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e65d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac154908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883bc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e8750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59db495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727beca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"Srikanth Ph: +1 (602) 784-6022 Email: srikanth.net1122@gmail.com PROFESSIONAL SUMMARY\"\n",
    "a=Parser()    \n",
    "b=a.extract_name(data)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8401fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "            \n",
    "def extract_skills(self,skills):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(skills)\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]          # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]         # remove the punctuation\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))  # generate bigrams and trigrams\n",
    "    found_skills = set()          # we create a set to keep the results in.\n",
    "    for token in filtered_tokens:             # we search for each token in our skills database\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "        for ngram in bigrams_trigrams:       # we search for each bigram and trigram in our skills database\n",
    "            if ngram.lower() in SKILLS_DB:\n",
    "                found_skills.add(ngram)\n",
    "        return found_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef161621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FileFetcher as ff\n",
    "\n",
    "\n",
    "SKILLS_DB = ['linq','sql server','agile','scrun','waterfall','jams','azure','sql','jira','spotlight']\n",
    "resume_text=ff.FileFetcher.fetch_pdf_file(5)\n",
    "par=Parser()\n",
    "print(par.extract_name(resume_text))\n",
    "print(par.extract_mobile_number(resume_text))\n",
    "print(par.extract_email(resume_text))\n",
    "print(par.extract_skills(resume_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd0d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "def extract_text_from_doc(path_doc):\n",
    "    temp = docx2txt.process(path_doc)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
